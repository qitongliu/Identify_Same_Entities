{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution Project\n",
    "\n",
    "@author: ty2326, lz2459, and ql2257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install editdistance\n",
    "pip install pygeocoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import editdistance\n",
    "import random\n",
    "from collections import Counter\n",
    "from pygeocoder import Geocoder\n",
    "from sklearn import cross_validation\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = \"Prakhar/er-assignment/fs/Instabase%20Drive/files/datasets/\"\n",
    "FILES = {\n",
    "    \"foursquare_test\": \"foursquare_test_hard.json\",\n",
    "    \"locu_test\": \"locu_test_hard.json\",\n",
    "    \"foursquare_train\": \"foursquare_train_hard.json\",\n",
    "    \"locu_train\": \"locu_train_hard.json\"\n",
    "}\n",
    "\n",
    "foursquare_test = pd.read_json(ib.open(PATH + FILES[\"foursquare_test\"]))\n",
    "locu_test = pd.read_json(ib.open(PATH + FILES[\"locu_test\"]))\n",
    "foursquare_train = pd.read_json(ib.open(PATH + FILES[\"foursquare_train\"]))\n",
    "locu_train = pd.read_json(ib.open(PATH + FILES[\"locu_train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After briefly checking the components of the training data, we found out that,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address and Zipcode Data Cleaning and Formalization Using Google API\n",
    "\n",
    "*  The street addresses for each venue are not in same format, for example, some addresses are representing \"West\" with W. , while others are using \"West\". Therefore, we import 'Google Maps Geocoding API' to reformat all the existing street addresses for all four data sets, and update postal code based on them, in case there is any wrongly recorded postal code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_geocoder = Geocoder(api_key='AIzaSyCh5YB2E2dvUn57l5Xc2zZ0mjnul8VK4uw')\n",
    "# other google api key: AIzaSyAocsDNe7TnZppttVeMcDtdx3fTBOL0Z-A\n",
    "for i in locu_train.index:\n",
    "    street = locu_train.ix[i]['street_address'] + ', NY ' + locu_train.ix[i]['postal_code']\n",
    "    result = my_geocoder.geocode(street)\n",
    "    locu_train.ix[i, 'street_address'] = result.formatted_address.split(',')[0].split('#')[0].strip(' ')\n",
    "    if locu_train.ix[i]['postal_code'] == '':\n",
    "        locu_train.ix[i, 'postal_code'] = result.postal_code\n",
    "\n",
    "for i in locu_test.index:\n",
    "    street = locu_test.ix[i]['street_address'] + ', NY ' + locu_test.ix[i]['postal_code']\n",
    "    result = my_geocoder.geocode(street)\n",
    "    locu_test.ix[i, 'street_address'] = result.formatted_address.split(',')[0].split('#')[0].strip(' ')\n",
    "    if locu_test.ix[i]['postal_code'] == '':\n",
    "        locu_test.ix[i, 'postal_code'] = result.postal_code\n",
    "\n",
    "for i in foursquare_train.index:\n",
    "    street = foursquare_train.ix[i]['street_address'] + ', NY ' + foursquare_train.ix[i]['postal_code']\n",
    "    result = my_geocoder.geocode(street)\n",
    "    foursquare_train.ix[i, 'street_address'] = result.formatted_address.split(',')[0].split('#')[0].strip(' ')\n",
    "    if foursquare_train.ix[i]['postal_code'] == '':\n",
    "        foursquare_train.ix[i, 'postal_code'] = result.postal_code\n",
    "\n",
    "for i in foursquare_test.index:\n",
    "    street = foursquare_test.ix[i]['street_address'] + ', NY ' + foursquare_test.ix[i]['postal_code']\n",
    "    result = my_geocoder.geocode(street)\n",
    "    foursquare_test.ix[i, 'street_address'] = result.formatted_address.split(',')[0].split('#')[0].strip(' ')\n",
    "    if foursquare_test.ix[i]['postal_code'] == '':\n",
    "        foursquare_test.ix[i, 'postal_code'] = result.postal_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Missing Values for Postal Code and Geo Data\n",
    "\n",
    "*   There are some venues that do not contain street address and postal code. For convenience in later analysis, we chose the top 10 common postal code among every data set, and calculate the percentage amount of them.\n",
    "*  There is one pair of longitude and latitude in the training data sets is missing. We decided to give them a fake pair of longitude and latitude, which is about the average of all longitude and latitude respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "code_counter_fs = Counter(foursquare_train['postal_code']).most_common(10)\n",
    "code_cn_fs = [t[1] for t in code_counter_fs][1:]\n",
    "code_pct_fs = [i/float(sum(code_cn_fs))for i in code_cn_fs]\n",
    "pos_code_fs = [t[0] for t in code_counter_fs][1:]\n",
    "\n",
    "locu_train['longitude'].fillna(-73.9600)\n",
    "locu_train['latitude'].fillna(-40.7300)\n",
    "\n",
    "code_counter_lc = Counter(locu_train['postal_code']).most_common(10)\n",
    "code_cn_lc = [t[1] for t in code_counter_lc][1:]\n",
    "code_pct_lc = [i/float(sum(code_cn_lc))for i in code_cn_lc]\n",
    "pos_code_lc = [t[0] for t in code_counter_lc][1:]\n",
    "\n",
    "code_counter_fs_ts = Counter(foursquare_test['postal_code']).most_common(10)\n",
    "code_cn_fs_ts = [t[1] for t in code_counter_fs_ts][1:]\n",
    "code_pct_fs_ts = [i/float(sum(code_cn_fs_ts))for i in code_cn_fs_ts]\n",
    "pos_code_fs_ts = [t[0] for t in code_counter_fs_ts][1:]\n",
    "\n",
    "code_counter_lc_ts = Counter(locu_test['postal_code']).most_common(10)\n",
    "code_cn_lc_ts = [t[1] for t in code_counter_lc_ts][1:]\n",
    "code_pct_lc_ts = [i/float(sum(code_cn_lc_ts))for i in code_cn_lc_ts]\n",
    "pos_code_lc_ts = [t[0] for t in code_counter_lc_ts][1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Website Data Cleaning\n",
    "\n",
    "Then, we generalize all website forms by deleting \"http://\", \"www\", and \"com\" in all four data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in foursquare_train.index:\n",
    "    if foursquare_train.ix[i]['website'] != \"\":\n",
    "        item = foursquare_train.ix[i]['website']\n",
    "        item = item.replace(\"http://\", \"\")\n",
    "        item = item.replace(\"www.\", \"\")\n",
    "        item = item.rstrip(\".com\")\n",
    "        item = item.rstrip(\".com/\")\n",
    "        item = item.replace(\"https://\", \"\")\n",
    "        foursquare_train.set_value(i, 'website', item)\n",
    "\n",
    "for i in locu_train.index:\n",
    "    if locu_train.ix[i]['website'] != \"\":\n",
    "        item = locu_train.ix[i]['website']\n",
    "        item = item.replace(\"http://\", \"\")\n",
    "        item = item.replace(\"www.\", \"\")\n",
    "        item = item.rstrip(\".com\")\n",
    "        item = item.rstrip(\".com/\")\n",
    "        item = item.replace(\"https://\", \"\")\n",
    "        locu_train.set_value(i, 'website', item)\n",
    "\n",
    "for i in foursquare_test.index:\n",
    "    if foursquare_test.ix[i]['website'] != \"\":\n",
    "        item = foursquare_test.ix[i]['website']\n",
    "        item = item.replace(\"http://\", \"\")\n",
    "        item = item.replace(\"www.\", \"\")\n",
    "        item = item.rstrip(\".com\")\n",
    "        item = item.rstrip(\".com/\")\n",
    "        item = item.replace(\"https://\", \"\")\n",
    "        foursquare_test.set_value(i, 'website', item)\n",
    "\n",
    "for i in locu_test.index:\n",
    "    if locu_test.ix[i]['website'] != \"\":\n",
    "        item = locu_test.ix[i]['website']\n",
    "        item = item.replace(\"http://\", \"\")\n",
    "        item = item.replace(\"www.\", \"\")\n",
    "        item = item.rstrip(\".com\")\n",
    "        item = item.rstrip(\".com/\")\n",
    "        item = item.replace(\"https://\", \"\")\n",
    "        locu_test.set_value(i, 'website', item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone Number Data Formalization and Filling Missing Values\n",
    "\n",
    "We then check through all venues in all data sets, and,\n",
    "*   Besides the street addresses, the phone number in each venue are not in same format either. Therefore, we use regx to change them into the same format. \n",
    "* For those with phone number but in irrgular format, we use the regx above to change them into regular form\n",
    "* For those without phone number, we give each of them a fake phone number starting with \"212\"(the most common start in both training data sets) and followed by a 7-digit random number\n",
    "* For those without postal number, we give each of them a fake postal number which is chosen with probability(the percentage we calculated above) from the top 10 common postal codes in each data set independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_parser = re.compile('\\(([^ ]*)\\) ([^ ]*)\\-([^ ]*)')\n",
    "\n",
    "for i in foursquare_test.index:\n",
    "    fs_ph = foursquare_test.ix[i]['phone']\n",
    "    fs_code = foursquare_test.ix[i]['postal_code']\n",
    "    if fs_ph is not None:\n",
    "        phone_parser = re_parser.match(fs_ph)\n",
    "        if phone_parser is not None:\n",
    "            foursquare_test.set_value(i, 'phone', ''.join(phone_parser.groups()))\n",
    "    else:\n",
    "        foursquare_test.set_value(i, 'phone', '212' + str(random.randrange(1000000, 9999999)))\n",
    "\n",
    "    if fs_code is None:\n",
    "        foursquare_test.set_value(i, 'postal_code', np.random.choice(pos_code_fs_ts, 1, p=code_pct_fs_ts)[0])\n",
    "\n",
    "for i in locu_test.index:\n",
    "    lc_ph = locu_test.ix[i]['phone']\n",
    "    lc_code = locu_test.ix[i]['postal_code']\n",
    "    if lc_ph is None:\n",
    "        locu_test.set_value(i, 'phone', '212' + str(random.randrange(1000000, 9999999)))\n",
    "\n",
    "    if lc_code is None:\n",
    "        locu_test.set_value(i, 'postal_code', np.random.choice(pos_code_lc_ts, 1, p=code_pct_lc_ts)[0])\n",
    "        \n",
    "\n",
    "for i in foursquare_train.index:\n",
    "    fs_ph = foursquare_train.ix[i]['phone']\n",
    "    fs_code = foursquare_train.ix[i]['postal_code']\n",
    "    if fs_ph is not None:\n",
    "        phone_parser = re_parser.match(fs_ph)\n",
    "        if phone_parser is not None:\n",
    "            foursquare_train.set_value(i, 'phone', ''.join(phone_parser.groups()))\n",
    "    else:\n",
    "        foursquare_train.set_value(i, 'phone', '212' + str(random.randrange(1000000, 9999999)))\n",
    "\n",
    "    if fs_code is None:\n",
    "        foursquare_train.set_value(i, 'postal_code', np.random.choice(pos_code_fs, 1, p=code_pct_fs)[0])\n",
    "\n",
    "for i in locu_train.index:\n",
    "    lc_ph = locu_train.ix[i]['phone']\n",
    "    lc_code = locu_train.ix[i]['postal_code']\n",
    "    if lc_ph is None:\n",
    "        locu_train.set_value(i, 'phone', '212' + str(random.randrange(1000000, 9999999)))\n",
    "\n",
    "    if lc_code is None:\n",
    "        locu_train.set_value(i, 'postal_code', np.random.choice(pos_code_lc, 1, p=code_pct_lc)[0])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Export Cleaned Datasets\n",
    "\n",
    "Finally, we export all preprocessed data sets into four csv files \"locu_train.csv\", \"locu_test.csv\", \"foursquare_train.csv\" and \"foursquare_test.csv\" for later use. (It will take a while to generate all four csv files, you can use the files that we generated to check our later work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "username = \"Qitong\"\n",
    "repo = \"entity-resolution\"\n",
    "\n",
    "f1 = ib.open('/{0}/{1}/fs/Instabase%20Drive/final/locu_train.csv'.format(username,repo))\n",
    "locu_train.to_csv(f1, encoding='utf-8')\n",
    "f1.close()\n",
    "\n",
    "f2 = ib.open('/{0}/{1}/fs/Instabase%20Drive/final/locu_test.csv'.format(username,repo))\n",
    "locu_test.to_csv(f2, encoding='utf-8')\n",
    "f2.close()\n",
    "\n",
    "f3 = ib.open('/{0}/{1}/fs/Instabase%20Drive/final/foursquare_train.csv'.format(username,repo))\n",
    "foursquare_train.to_csv(f3, encoding='utf-8')\n",
    "f3.close()\n",
    "\n",
    "f4 = ib.open('/{0}/{1}/fs/Instabase%20Drive/final/foursquare_test.csv'.format(username,repo))\n",
    "foursquare_test.to_csv(f4, encoding='utf-8')\n",
    "f4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Feature Creation and Model Input Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Cleaned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATH = \"Qitong/entity-resolution/fs/Instabase%20Drive/final/\"\n",
    "FILES = {\n",
    "    \"foursquare_train\": \"foursquare_train.csv\",\n",
    "    \"foursquare_test\": \"foursquare_test.csv\",\n",
    "    \"locu_train\": \"locu_train.csv\",\n",
    "    \"locu_test\": \"locu_test.csv\",\n",
    "    \"matches\": \"matches_train_hard.csv\"\n",
    "}\n",
    "\n",
    "locu_train = pd.read_csv(ib.open(PATH + FILES['locu_train']))\n",
    "locu_test = pd.read_csv(ib.open(PATH + FILES['locu_test']))\n",
    "foursquare_train = pd.read_csv(ib.open(PATH + FILES['foursquare_train']))\n",
    "foursquare_test = pd.read_csv(ib.open(PATH + FILES['foursquare_test']))\n",
    "\n",
    "matches = pd.read_csv(ib.open(PATH + FILES['matches']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Matched id Pairs\n",
    "\n",
    "In order to locate the id in each matched pair in the training data sets easier, we combine each id in the mactched pairs and the index of its venue in the two training data sets respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matched_pairs = zip(list(pd.match(matches['foursquare_id'], foursquare_train['id'])),\n",
    "                    list(pd.match(matches['locu_id'], locu_train['id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the existing features are hard to compare with each other, we decide to use them to create some new features to make the classification problem simpler. \n",
    "Additionaly, due to the large number amount of data and we do not want our model to learn every single pair of the samples and test every pair later, we decide to select a relatively small amount of pairs to train the model, and also ignore  pairs that are very unlikely to be a match for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Testing Dataset\n",
    "\n",
    "We first start to create the training data frame that we will put into our model later. \n",
    "For all the training samples we have, we decide that, for each pair of venues from locu_train and foursquare_train, we create the following seven features. They are, \n",
    "1. the editdistance between names\n",
    "2. the editdistance bewteen the string of phone numbers\n",
    "3. the editdistance between the string of street addresses (all letters are in lower case) \n",
    "4. the editdistance between the string of websites if both of them exist (if not, we set the distance equals to 30)\n",
    "5. the absolute value of difference between longitudes\n",
    "6. the absolute value of difference between latitudes\n",
    "7. the absolute value of differnece between postal codes\n",
    "\n",
    "After creating the seven features each time, we start to decide whether we want to put this pair into our consideration or not.\n",
    "\n",
    "Here, we will not consider the pair as a possible match if none of the following condition is satisified. They are,\n",
    "* the editdistance between names is greater than 10\n",
    "* the editdistance between phone numbers is greater than 3\n",
    "* the absolute value of difference between longitudes is greater than 0.0001 and that between latitudes is greater than 0.001\n",
    "* the editdistance between websites is greater than 10\n",
    "* the editdistance between street addresses is greater than 8\n",
    "\n",
    "Otherwise, we will add the list of its seven features along with the index pair of the two venues into our result_test data frame for later classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_test = pd.DataFrame(columns=['name_dis', 'phone_dis', 'geo_log_dis', 'geo_lat_dis', 'poscode_dis', 'add_dis', 'web_dis', 'index_pair'])\n",
    "\n",
    "for i in foursquare_test.index:\n",
    "    for j in locu_test.index:\n",
    "        cur_fs = foursquare_test.ix[i]\n",
    "        cur_lc = locu_test.ix[j]\n",
    "        name_dis = editdistance.eval(cur_fs['name'], cur_lc['name'])\n",
    "        phone_dis = editdistance.eval(str(cur_fs['phone']), str(cur_lc['phone']))\n",
    "        add_dis = editdistance.eval(str(cur_fs['street_address']).lower(), str(cur_lc['street_address']).lower())\n",
    "\n",
    "        if cur_fs['website'] and cur_lc['website']:\n",
    "            web_dis = editdistance.eval(str(cur_fs['website']), str(cur_lc['website']))\n",
    "        else:\n",
    "            web_dis = 30\n",
    "\n",
    "        geo_log_dis = abs(cur_lc['longitude'] - cur_fs['longitude'])\n",
    "        geo_lat_dis = abs(cur_lc['latitude'] - cur_fs['latitude'])\n",
    "\n",
    "        poscode_dis = abs(int(cur_fs['postal_code']) - int(cur_lc['postal_code']))\n",
    "\n",
    "        #to be tuned\n",
    "        if name_dis <= 10 or phone_dis <= 3 or (geo_log_dis <= 0.0001 and geo_lat_dis<= 0.001) or web_dis <= 10 or add_dis <= 8:\n",
    "            index_pair = (i, j)\n",
    "            feature_list = [name_dis, phone_dis, geo_log_dis, geo_lat_dis, poscode_dis, add_dis, web_dis, index_pair]\n",
    "            result_test.loc[len(result_test.index)] = feature_list\n",
    "            \n",
    "\n",
    "username = \"Qitong\"\n",
    "repo = \"entity-resolution\"\n",
    "\n",
    "f2 = ib.open('/{0}/{1}/fs/Instabase%20Drive/final/test1.csv'.format(username,repo))\n",
    "result_test.to_csv(f2)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Training Dataset\n",
    "\n",
    "First, we check if this pair is a match or not by checking the index pair with the \"matched_pairs\" list we made previously. \n",
    "* If it is a match, we add the list of all seven features along with a label \"1\" into the result_train data frame.\n",
    "* If it is not a match, but the editdistance of names is not greater than 3 and the editdistance of phone number is not greater than 2, we also add the features and a label \"1\" into the result_train data frame.\n",
    "\n",
    "If neither of the above two conditions is satisfied, we then check the following three conditions,\n",
    "* The editdistance between names is not greater than 7\n",
    "* The editdistance between phone numbers is not greater than 3\n",
    "* The absolute value of difference between longitudes is not greater than 0.0001 and that between latitudes is not greater than 0.001\n",
    "\n",
    "If the pair satisfies any one of the three conditions, we add its seven features along with a label \"0\" into the result_train data frame, to indicate that even they are similar enough in some features, they are not a match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLEASE DO NOT RUN THE FOLLOWING CODE TO GENERATE ANOTHER TRAINING DATASET!!\n",
    "\n",
    "### here we comment the code below because we tuned the paramaters in RandomForest Model based on the sepecific train dataset in next step. This should not be consider as a unreasonable request. If you insist, pls contact us.\n",
    "\n",
    "(In the step of data cleaning, we semi-randomly generated zip and phone numbers to replace NAs in the original dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# result_train = pd.DataFrame(columns=['name_dis', 'phone_dis', 'geo_log_dis', 'geo_lat_dis', 'poscode_dis', 'add_dis', 'web_dis', 'label'])\n",
    "\n",
    "# for i in foursquare_train.index:\n",
    "#     for j in locu_train.index:\n",
    "#         cur_fs = foursquare_train.ix[i]\n",
    "#         cur_lc = locu_train.ix[j]\n",
    "#         name_dis = editdistance.eval(cur_fs['name'], cur_lc['name'])\n",
    "#         phone_dis = editdistance.eval(str(cur_fs['phone']), str(cur_lc['phone']))\n",
    "#         add_dis = editdistance.eval(str(cur_fs['street_address']).lower(), str(cur_lc['street_address']).lower())\n",
    "\n",
    "#         if cur_fs['website'] and cur_lc['website']:\n",
    "#             web_dis = editdistance.eval(str(cur_fs['website']), str(cur_lc['website']))\n",
    "#         else:\n",
    "#             web_dis = 30\n",
    "\n",
    "#         geo_log_dis = abs(cur_lc['longitude'] - cur_fs['longitude'])\n",
    "#         geo_lat_dis = abs(cur_lc['latitude'] - cur_fs['latitude'])\n",
    "\n",
    "#         poscode_dis = abs(int(cur_fs['postal_code']) - int(cur_lc['postal_code']))\n",
    "\n",
    "#         #to be tuned \n",
    "#         if (i, j) in matched_pairs or (name_dis<=3 and phone_dis<=2):\n",
    "#             feature_list = [name_dis, phone_dis, geo_log_dis, geo_lat_dis, poscode_dis, add_dis, web_dis, 1]\n",
    "#             result_train.loc[len(result_train.index)] = feature_list\n",
    "#         elif name_dis <= 7 or phone_dis <= 3 or (geo_log_dis <= 0.0001 and geo_lat_dis<= 0.001):\n",
    "#             feature_list = [name_dis, phone_dis, geo_log_dis, geo_lat_dis, poscode_dis, add_dis, web_dis, 0]\n",
    "#             result_train.loc[len(result_train.index)] = feature_list\n",
    "\n",
    "            \n",
    "# username = \"Qitong\"\n",
    "# repo = \"entity-resolution\"\n",
    "\n",
    "# f1 = ib.open('/{0}/{1}/fs/Instabase%20Drive/final/result_should_not_be_created.csv'.format(username,repo))\n",
    "# result_train.to_csv(f1)\n",
    "# f1.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run RandomForest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Final Training and Testing Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATH = \"Qitong/entity-resolution/fs/Instabase%20Drive/final/\"\n",
    "FILES = {\n",
    "    \"result\": \"result.csv\",\n",
    "    \"test\": \"test.csv\",\n",
    "    \"foursquare_test\": \"foursquare_test.csv\",\n",
    "    \"locu_test\": \"locu_test.csv\"\n",
    "}\n",
    "\n",
    "train = pd.read_csv(ib.open(PATH + FILES['result']))\n",
    "test = pd.read_csv(ib.open(PATH + FILES['test']))\n",
    "locu_test = pd.read_csv(ib.open(PATH + FILES['locu_test']))\n",
    "foursquare_test = pd.read_csv(ib.open(PATH + FILES['foursquare_test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that there is one venue in the test data that has NA in longitute difference and latitute difference, therefore we randomly set a pair of values to it to avoid the occurance of error while doing prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.fillna({'geo_log_dis': 0.003, 'geo_lat_dis': 0.02}, inplace=True)\n",
    "train = train.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = train.columns[[1,2,3,4,5,6,7]]\n",
    "y = train['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Cross Validation to Find the Best Parameters for the Model Based on THE SEPECIFIC TRAINING DATASET!\n",
    "\n",
    "##### Since it takes a long time, please feel free to skip this part when checking our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "#     train[features], y, test_size=0.4, random_state=0)\n",
    "\n",
    "# tuned_parameters = [{'n_estimators': [40, 80, 110, 130], 'max_depth': [6, 8, 10, 15],\n",
    "#                      'max_features': [4,6,7]}]\n",
    "\n",
    "\n",
    "# scores = ['precision', 'recall']\n",
    "\n",
    "# for score in scores:\n",
    "#     print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "#     print()\n",
    "\n",
    "#     clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5,\n",
    "#                        scoring='%s_weighted' % score)\n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "#     print(\"Best parameters set found on development set:\")\n",
    "#     print()\n",
    "#     print(clf.best_params_)\n",
    "#     print()\n",
    "#     print(\"Grid scores on development set:\")\n",
    "#     print()\n",
    "#     for params, mean_score, scores in clf.grid_scores_:\n",
    "#         print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#               % (mean_score, scores.std() * 2, params))\n",
    "\n",
    "#     print(\"Detailed classification report:\")\n",
    "#     print()\n",
    "#     print(\"The model is trained on the full development set.\")\n",
    "#     print(\"The scores are computed on the full evaluation set.\")\n",
    "#     print()\n",
    "#     y_true, y_pred = y_test, clf.predict(X_test)\n",
    "#     print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model and Find the Most Important Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators= 130, max_depth=7)\n",
    "clf.fit(train[features], y)\n",
    "\n",
    "print clf.feature_importances_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Using Testing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to accomplish our pair match task, we need to first, find the predicted class probabilities of each test input, and based on the probabilities, to decide which pairs are possibily labeled as \"1\" i.e. a match, and which are definitely not; then, for those having the possibility to become an actual match, if there are some venues have more than one match pair, we need to decide which one to pair with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Predicted Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_test_prob = clf.predict_proba(test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Match Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the Match Pairs with High Predicted Probability\n",
    "\n",
    "First of all, we initialize the maximum probability to be 0.78, the predicted index to be None, and the index of the first venue in the pair we are checking to be 0. Then, we iterate through all possible pairs to see if the class probability is no less than the maximum probability or not.\n",
    "* If yes, we then update the maximum probability by the class probability, and update the predicted index by the index  of the second venue in its original data set.\n",
    "* If not, we pass to the next pair directly.\n",
    "\n",
    "When the index of the first venue in this pair does not equal to the index we initialized, all (0, x) pairs are checked. At this time, the value saved under \"pred_ind\" is the index of the venue that is the most possible match with the first venue in the first data set.\n",
    "\n",
    "Now, we can add this pair of indices into our prediction list, and record both indices as 'paired' by adding them into paired_index lists separately. \n",
    "Then, we increase the index variable by one, set the prediced index varialbe and maximum probability variable back to their initial value, and finally, continue our iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = 0\n",
    "max_prob = 0.78\n",
    "pred_ind = None\n",
    "pred_list = []\n",
    "paired_id0 = []\n",
    "paired_id1 = []\n",
    "\n",
    "for i in xrange(len(preds_test_prob)):\n",
    "    index_list = test.ix[i]['index_pair'].strip('(').strip(')').split(', ')\n",
    "    if int(index_list[0]) == ind:\n",
    "        if preds_test_prob[i][1] >= max_prob:\n",
    "            max_prob =  preds_test_prob[i][1]\n",
    "            pred_ind = int(index_list[1])\n",
    "    else:\n",
    "        if pred_ind is not None:\n",
    "            pred_list.append((ind, pred_ind))\n",
    "            paired_id0.append(ind)\n",
    "            paired_id1.append(pred_ind)\n",
    "        ind += 1\n",
    "        max_prob = 0.78\n",
    "        pred_ind = None\n",
    "        if preds_test_prob[i][1] >= max_prob:\n",
    "            max_prob = preds_test_prob[i][1]\n",
    "            pred_ind = int(index_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the iteration process, we are pretty sure that the pairs stored in \"pred_list\" are the most possible matches in both data sets, therefore, we can now add their ids into our final match list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches_test = []\n",
    "for p in pred_list:\n",
    "    matches_test.append([locu_test.ix[p[1]]['id'], foursquare_test.ix[p[0]]['id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid the situation that one id appears more than once in the final match list, we store all found ids for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match_lc_ids = [match[0] for match in matches_test]\n",
    "match_fs_ids = [match[1] for match in matches_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the Match Pairs with Midium Predicted Probability\n",
    "\n",
    "\n",
    "So far we have found all pairs that our classifer predicts as a match with a relatively high probability. Now, we have to deal with pairs that do not have high predicted class probabilities and to find if there are any matches among them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we first construct a 400x400 matrix of zeroes, and then update its elements if the predicted probability of the pairs that neither are included in the final match list, nor the two indices are recorded already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_prob_matrix = np.zeros((400, 400))\n",
    "\n",
    "for i in xrange(len(preds_test_prob)):\n",
    "    index_list = test.ix[i]['index_pair'].strip('(').strip(')').split(', ')\n",
    "    fs_ind = int(index_list[0])\n",
    "    lc_ind = int(index_list[1])\n",
    "    if fs_ind not in paired_id0 and lc_ind not in paired_id1:\n",
    "        preds_prob_matrix[fs_ind, lc_ind] = preds_test_prob[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the probability matrix, we first iterate through rows (each row represents a venue in \"locu\"), and for each row, we add its index as a key to dictonary \"top_5_row\". After finding the top 5 probabilities that are in the range of 0.5 and 0.78 in this row, we store their indices into the dictionary under the key we just added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_5_row = {}\n",
    "for index in xrange(preds_prob_matrix.shape[0]):\n",
    "    arr = preds_prob_matrix[index,:]\n",
    "    elem = []\n",
    "    sorted_index = np.argsort(arr)[::-1]\n",
    "    if sorted_index[0] == 0:\n",
    "        top_5_row[index] = elem\n",
    "        continue\n",
    "\n",
    "    for j in sorted_index:\n",
    "        if arr[j] < 0.78 and arr[j] > 0.5:\n",
    "            elem.append(j)\n",
    "\n",
    "    top_5_row[index] = elem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing through all rows of the matrix, we do the same thing for each of its columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_5_col = {}\n",
    "for index in xrange(preds_prob_matrix.shape[1]):\n",
    "    arr = preds_prob_matrix[:,index]\n",
    "    elem = []\n",
    "    sorted_index = np.argsort(arr)[::-1]\n",
    "    if sorted_index[0] == 0:\n",
    "        top_5_col[index] = elem\n",
    "        continue\n",
    "\n",
    "    for i in sorted_index:\n",
    "        if arr[i] < 0.78 and arr[i] > 0.5:\n",
    "            elem.append(i)\n",
    "\n",
    "    top_5_col[index] = elem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have two dictionaries of probabilities for two test data sets respectively.\n",
    "\n",
    "We then iterate through the \"top_5_row\" dictionary. For each key/value pair, for each index in the value-list, we look up its corresponding index along with its value-list in the \"top_5_col\" dictionary, if the index is also in the value-list, we say this pair of indices is a possible match and store their corresponding ids as \"match_id\". \n",
    "\n",
    "If this pair of ids is not in the final match list, to avoid repetition, we check if the second index has already appeared in a possible pair or not. Here, if it is the first time we see this index, we add the id pair into the check dictionary, \"matches_sec\", under the indices pair key. If it has already appeared before, we compare its predicted class probability with the existing pair's, and update the pair if its probability is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches_sec = {}\n",
    "for i, top_vals in top_5_row.items():\n",
    "    for ind_i, j in enumerate(top_vals):\n",
    "        top_vals_j = top_5_col[j]\n",
    "        if i in top_vals_j:\n",
    "            match_id = [locu_test.ix[j]['id'], foursquare_test.ix[i]['id']]\n",
    "            if match_id not in matches_test:\n",
    "                sec_keys = [key[1] for key in matches_sec.keys()]\n",
    "                if j not in sec_keys:\n",
    "                    matches_sec[(i,j)] = match_id\n",
    "                else:\n",
    "                    key_j = [key for key in matches_sec.keys() if key[1] == j]\n",
    "                    pre_i = key_j[0][0]\n",
    "                    if preds_prob_matrix[pre_i, j] < preds_prob_matrix[i, j]:\n",
    "                        del matches_sec[(pre_i, j)]\n",
    "                        matches_sec[(i,j)] = match_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After doing all processes above, we have a dictionary with pairs of indices as keys and their corresponding pairs of ids as values.\n",
    "\n",
    "Before finally adding the pair into our final match list, we check again to make sure that both indices in the pair are not in our match list already. After checking, we add those satisfied pairs into our final match list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for match in matches_sec.values():\n",
    "    if match[0] not in match_lc_ids and match[1] not in match_fs_ids:\n",
    "        matches_test.append(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Final Missing Pairs\n",
    "\n",
    "#### Can skip this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match_lc_ids_f = [match[0] for match in matches_test]\n",
    "match_fs_ids_f = [match[1] for match in matches_test]\n",
    "\n",
    "add_len = 0\n",
    "\n",
    "for i in xrange(len(test)):\n",
    "    cur_name_dis = test.ix[i]['name_dis']\n",
    "    cur_ph_dis = test.ix[i]['phone_dis']\n",
    "    cur_web_dis = test.ix[i]['web_dis']\n",
    "    cur_geo_dis = test.ix[i]['geo_log_dis'] + test.ix[i]['geo_lat_dis']\n",
    "    if (cur_name_dis <= 4 and cur_geo_dis <= 0.0005) or cur_ph_dis <= 1 or (cur_web_dis != 0 and cur_web_dis <= 2):\n",
    "        index_list = test.ix[i]['index_pair'].strip('(').strip(')').split(', ')\n",
    "        fs_ind = int(index_list[0])\n",
    "        lc_ind = int(index_list[1])\n",
    "        add_id_pair = [locu_test.ix[lc_ind]['id'], foursquare_test.ix[fs_ind]['id']]\n",
    "        if add_id_pair[0] not in match_lc_ids_f and add_id_pair[1] not in match_fs_ids_f:\n",
    "            add_len += 1\n",
    "            matches_test.append(add_id_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Output Matches into csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "username = \"Qitong\"\n",
    "repo = \"entity-resolution\"\n",
    "\n",
    "f = ib.open('/{0}/{1}/fs/Instabase%20Drive/final/matches_test.csv'.format(username,repo))\n",
    "f.write('locu_id, foursquare_id\\n')\n",
    "\n",
    "for match in matches_test:\n",
    "    f.write(','.join(match) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
